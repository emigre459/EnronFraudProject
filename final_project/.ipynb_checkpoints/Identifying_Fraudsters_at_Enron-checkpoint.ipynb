{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of the Udacity Data Analyst Nanodegree Machine Learning Project\n",
    "\n",
    "This project's goal is to build an algorithm (and data workflow) that can predict whether a person from our Enron emails and financial data was likely a Person of Interest (POI) in the investigation of Enron's fraud. We have a manually developed list of POIs, provided by Udacity and created from news reports at the time, that represents ground truth and that we can use for testing purposes (this ground truth is represented within the data already as the value for `poi`).\n",
    "\n",
    "As part of this project, there are a series of questions that I'll answer to show how I'm thinking about this and what direction I've taken my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "The data provided are not (as I had originally expected) based upon the text corpus of all the emails, but rather are high-level summary data aggregated at the individual person level. Specifically, for each of the 146 people for whom we have data, the data are represented by a single dictionary in python, with the top-level keys of the dict being the name of each person in the data set in the format `\"LastName FirstName MiddleInitial\"`. For each person, their data are organized in three primary categories (the strings shown here are all dictionary keys for the dictionary associated with a person's name, with the values for each of these lower-level dict keys being the value of that item, e.g. salary - this text is taken from the Udacity project details page):\n",
    "\n",
    "1. **Financial features:** `['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees']` (all units are in US dollars)\n",
    "\n",
    "2. **Email features:** `['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']` (units are generally number of emails messages and therefore integers; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "3. **POI label:** `[‘poi’]` (boolean, represented as integer 0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing and Formatting\n",
    "\n",
    "Since the project assumes we'll be manipulating data in a dictionary format and I'd like to work in a pandas DataFrame, I'll need to use some of the pre-made code from the project to get the data into the DataFrame format I know and love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "#Let's grab all of the features so we can explore them a bit before deciding which ones to keep.\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', \n",
    "                 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', \n",
    "                 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'director_fees', \n",
    "                 'to_messages', 'email_address', 'from_poi_to_this_person', \n",
    "                 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values are currently being indicated as `\"NaN\"` instead of as `np.NaN` and as a result any of our typical `isnull()` approaches won't work and other methods/functions won't be able to automatically ignore these values either. Let's fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Replace \"NaN\" with np.NaN\n",
    "df.replace(to_replace = \"NaN\", value = np.NaN, inplace = True)\n",
    "df.index.rename(\"Name\", inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Cleaning and Counting\n",
    "Here we'll take a look at the null values, make sure the signs of the data are all correct and meaningful, and do other basic cleaning to make sure these data can be processed easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many null values in total df?\n",
    "print \"Number of null/NaN/etc. values in dataframe =\", df.isnull().sum().sum()\n",
    "\n",
    "#How many rows (with total of 21 columns) are totally null?\n",
    "print \"Number of rows with ONLY null/NaN/etc. values in dataframe =\", df.isnull().all(axis = 0).sum()\n",
    "\n",
    "#How many columns (with 146 rows) are totally null?\n",
    "print \"Number of columns with ONLY null/NaN/etc. values in dataframe =\", df.isnull().all(axis = 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have any records/people with a large fraction of null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#How many nulls are there per row as a percentage of all the available features?\n",
    "(df.isnull().sum(axis = 1)/len(df.columns)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoa.** There are a lot of people in this data set that have more than 75% null features (~16 out of 21 features missing). When you consider that a number of the features are also highly correlated most likely (e.g. it looks like some of these are debits that match income in other features), this isn't a lot to go on. Ultimately, I'm going to keep these records, as we really don't have a lot of data to work with already, but I'll definitely consider coming back and trimming out these records if they're dragging down precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#How many nulls are there per column as a percentage of all the people for whom we have records?\n",
    "(df.isnull().sum(axis = 0)/len(df)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like we know very little about loan advances, director fees, and deferred restricted stock.** Each of these features has more than 85% of its records as null values, which translates to less than 29 records actually being non-null. I'm going to drop these features entirely due to the lack of data points and the likelihood that any classifier is going to provide those with little or no weight, but the rest I'll keep. I'll also store a copy of the DataFrame with those features still included, just so I can make sure to compare the results when doing feature selection tests (e.g. `SelectKBest`) later and make sure I haven't made a bad choice.\n",
    "\n",
    "Given that we have no records with no missing values, and none of our features (with the exception of `poi`) are completely non-null, there aren't a lot of options here. Often missing values can be dealt with via listwise deletion wherein the records with any null values are removed, but that would remove all of our data from this data set and is thus unacceptable. While statistical (e.g. regression-based) or machine-learning-based methods for imputation could be utilized, I'd rather compare the performance of our selected model when keeping all values, removing features with lots of missing values, and removing records with lots missing values first. If sufficient performance can be achieved in this manner, we'll consider our work done. But if we can't achieve sufficient performance via this combination of approaches, we'll consider alternative imputation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowNull = df.drop(columns = ['loan_advances', 'director_fees', 'restricted_stock_deferred'])\n",
    "df_lowNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = df.isnull().sum().sum()\n",
    "#Treats all nonzero and np.nan values as True, so only zero values are counted as False\n",
    "zero_count = df.size - df.astype(bool).sum().sum()\n",
    "\n",
    "print \"Fraction of all data points that are missing =\", float(null_count + zero_count)/df.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, so technically this isn't a sparse matrix** by traditional definitions (when considering both null and zero values). But it is certainly close! Still, that's good to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at unique values in the columns. If a feature has only one value, it's not useful for predicting anything at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(pd.Series.nunique).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The only items that stick out here are `poi`, which we know is our binary label and thus 2 is an appropriate value, and `loan_advances` which we already know is missing the vast majority of its data.** So nothing need be done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing: **let's check to make sure that our index labels make sense.** Since this isn't a data set that I created myself, but rather the one given me by Udacity, let's make sure there aren't any non-sensical names for these records. Since this is a relatively small list, we'll just do a close-read to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well that doesn't seem quite right.** I'm going to make a leap of logic here and assume that `TOTAL` and `THE TRAVEL AGENCY IN THE PARK` aren't likely individual people and are thus less than helpful records. Let's take a quick look at their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible, I suppose, that an organization as opposed to an entity could be committing fraud, we're specifically looking for POIs: *persons* of interest. So we'll drop this record (especially given how little data it has)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['TOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly (especially since we already looked at this particular issue in the classes earlier), this entry has some of the biggest financial values of the whole data set. As we know from earlier in the class, this is because these data came from a spreadsheet that included a `TOTAL` row for totaling up all of the financial data. Since this is a spreadsheet quirk, we can remove it from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace = True)\n",
    "df_lowNull.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we already found two records that were either questionable in nature or just clearly incorrect to include, let's do a quick regex on our names. In theory, we should see only 1-2 spaces in each index label (for people without an initial and those with one, resp.), so we can use this for our searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex pattern =\n",
    "    #alphanumeric, followed by whitespace, followed by alphanumeric: can be found once or twice only\n",
    "df.index.str.contains(r'\\w\\s\\w{1,2}').sum() == len(df.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, we're good to go, it looks like only the \"names\" we already found were problematic according to these rules.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "OK, now that we've done some basic investigation and cleaning, let's take a look at our univariate distributions for each feature. Note that we'll be plotting everything here both as one data set and faceted by class (`poi` value) to determine if there are any obvious trends/issues between the two classes.\n",
    "\n",
    "We'll first focus on the features that we are planning to drop due to their high fraction of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loan_advances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#sns.distplot(df['loan_advances'].dropna(), fit=norm, kde=False)\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "sns.distplot(df['loan_advances'].dropna(), kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the small number of data points and the ludicrous spread in the data, this, unsurprisingly, isn't providing us much info. Let's quickly facet by our POI label, just to be thorough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['loan_advances'].notnull()][['loan_advances','poi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"poi\")\n",
    "g = g.map(sns.distplot, \"loan_advances\", kde = False)\n",
    "g.set(xlim=(0, 1E8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that `poi` labels are split nearly evenly (2:1) between the few records we have that are non-null for `loan_advances`, but the amount of missing values for this feature is still overwhelming and likely to not help our prediction ability if the feature is allowed to remain. That being said, the significant difference in values between the two labels is a clear trend, so as stated before we'll certainly test our predictions both with and without this feature and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### director_fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['director_fees'].dropna(), kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"poi\")\n",
    "g = g.map(sns.distplot, \"director_fees\", kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no obvious outliers in this one** and there's no POIs with non-null values for this feature. Also, the count of data points is low enough (and spread in value enough) that I don't see any value in performing transformations on this, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restricted_stock_deferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['restricted_stock_deferred'].dropna(), kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"poi\")\n",
    "g = g.map(sns.distplot, \"restricted_stock_deferred\", kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've clearly got an outlier on this one,** albeit not an interesting outlier (in that it isn't a POI). Let's check it out though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['restricted_stock_deferred'] > 2.5E6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this may be a really high value for deferred restricted stock, it's not inherently suspicious or an obvious flaw in the data. So we'll leave it be, as it may be usefully predictive (although unlikely, since we know that this individual outlier is labeled as not being a POI - but since we could one day use this classifier on new data, it may be a mistake to remove this outlier as it may be useful in future classifications, if we were to do them).\n",
    "\n",
    "**Ultimately, it doesn't seem like these three features are crucial to our predictive modeling.** As I said before, the safest bet is to treat this empirically, and see how our precision and recall fair when we both include and exclude this features with lots of missing values, but my intuition at the moment is that they won't improve the results significantly (and may run the risk of causing actual detriment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### poi\n",
    "We already know from our earlier exploration that it only takes on two values (as it should) and that it is present for all records. Plus, we're already faceting all of our exploratory visualizations by this feature. Still, let's do a quick visualization to get a sense of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Fraction of poi's in the dataset =\", float(df['poi'].value_counts()[1])/float(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x ='poi', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email_address\n",
    "I won't visualize this in a traditional manner, but I will look at the counts of unique values for this feature. I may be interested in using it for later feature engineering and I want to make sure that, where it isn't NaN, each entry has a unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['email_address'].dropna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are duplicate, if any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df['email_address'].dropna().value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are @enron.com?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_address'].dropna().str.contains(r'@enron.com').sum() == len(df['email_address'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, nevermind on my feature engineering idea. All of these are @enron.com email addresses. Bah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['bonus'].dropna(),kde=False, bins = 50)\n",
    "#plt.xticks(rotation=90)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"poi\")\n",
    "g = g.map(sns.distplot, \"bonus\", kde = False, bins = 50)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a reasonable distribution that applies to both `poi` labels! It's interesting to note here that I might have expected high bonuses to cluster around POIs, which they do somewhat, but the maximum bonus ($8M) goes to a non-POI! Obviously, our model will have to be smarter than my intuition to have high predictive validity.\n",
    "\n",
    "Let's take a quick look at those high-bonus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['bonus'] > 5E6][['bonus','poi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or perhaps I wasn't so off-base in my intuition after all!** It looks like 75% of the folks with bonuses above $5M are POIs. Interesting...\n",
    "\n",
    "For the purposes of later modeling, let's do one more thing: take a look at how normally distributed these data may be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['bonus'].dropna(),kde=False, bins = 50, fit = norm)\n",
    "g.set_xscale('log')\n",
    "g.set_xlim([5E4,1E7])\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be useful to know for later: it looks like even a $log_{10}$ transform of `bonus` doesn't produce a particularly Gaussian distribution, suggesting that I should stay away from machine learning (ML) models that require Gaussian assumptions for the features (e.g. linear models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deferral_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['deferral_payments'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"deferral_payments\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much to see here, this feature in particular doesn't show any obvious trend for poi vs. non-poi data. Let's quickly take a look at that outlier of $6M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['deferral_payments','poi']].dropna().sort_values(by='deferral_payments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['FREVERT MARK A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing particularly of note about Mark Frevert, so let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deferred_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['deferred_income'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"deferred_income\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the last feature, not much of note here, although POIs do seem a bit more spread out here. \n",
    "\n",
    "From here on in, I'll plan to only comment when I see something of particular note in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exercised_stock_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['exercised_stock_options'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"exercised_stock_options\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['expenses'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"expenses\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['from_messages'].dropna(),kde=False, bins = 25)\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"from_messages\", kde = False, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['from_messages'] > 1E4][['to_messages','from_messages']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We have someone that's sent more than 14,000 messages?! That's a whole lot of emails, especially when compared to the rest of the dataset. Let's see if anything pops up for Vince in `to_messages` that may contextualize this (e.g. he also gets lots of messages to him, suggesting he might just be a popular guy and there's no data entry issues here or anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['to_messages'].dropna(),kde=False, bins = 25)\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"to_messages\", kde = False, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['to_messages'] > 1E4][['to_messages','from_messages']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results suggest to me that it's not crazy to have a 2-3x multiplier when comparing `to_messages` to `from_messages` (at least for these 3 people), so I'm not going to worry too much about it in terms of data quality. It seems to indicate that frequency of communication isn't a very useful metric for identifying POIs though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_poi_to_this_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['from_poi_to_this_person'].dropna(),kde=False, bins = 25)\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"from_poi_to_this_person\", kde = False, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['from_poi_to_this_person'] > 400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_this_person_to_poi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['from_this_person_to_poi'].dropna(),kde=False, bins = 25)\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"from_this_person_to_poi\", kde = False, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['from_this_person_to_poi'] > 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not entirely sure what to make of this and the preceding feature, but I don't see anything standing out in a bad way that would impact our analysis. It's curious that John Lavorato would be receiving large amounts of emails from POIs and be sending a similar amount to them, while David Delainey is only sending POIs emails without receiving many in return. But perhaps that's a reflection of their job roles? For example, John might be a lawyer for Enron and his emails may solicit frequent and immediate replies, whereas David may be an internal auditor who is often sending status reports to POIs but who only gets responses when something bad shows up (although, this being Enron we're talking about, perhaps that's a bad example...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shared_receipt_with_poi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['shared_receipt_with_poi'].dropna(),kde=False, bins = 25)\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"shared_receipt_with_poi\", kde = False, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['shared_receipt_with_poi']>4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long_term_incentive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['long_term_incentive'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"long_term_incentive\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['long_term_incentive'] > 3E6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['other'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"other\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['other'] > 6E6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restricted_stock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['restricted_stock'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"restricted_stock\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### salary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['salary'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"salary\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['salary'] > 7.5E5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Of all of the financial data we've looked at so far, these are the only data that look even vaguely normally-distributed. I wonder why that is? Likely because a salary is something people of many different levels within an organization typically have, while specialty categories like restricted stocks are often reserved for only higher-level employees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_payments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['total_payments'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"total_payments\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['total_payments'] > 6E7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of payments from the company to Kenny boy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_stock_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(df['total_stock_value'].dropna(),kde=False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "p = sns.FacetGrid(df, col=\"poi\")\n",
    "p = p.map(sns.distplot, \"total_stock_value\", kde = False, bins = 10)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['total_stock_value']>3E7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I think it's fair to say that we've seen quite a mixture of results for many of these variables. I expected there to be many more clear trends in the financial data for POIs vs. non-POIs (primarily that POIs were always in the top 10% for a given financial feature) but that only proved to be true part of the time. I'm still going to use this hypothesis about financial data for feature engineering, as I *did* see this pattern holding true for some features (and thus it may still be something that survives my feature selection step), but we'll just have to run the experiment to see.\n",
    "\n",
    "Data related to email counts were similarly mixed in results. It seemed as though I more consistently saw at least one POI in the highest levels of the distributions, but rarely was it entirely POIs that I observed. Clearly this is a good data set for modeling, given how human intuition related to these features is only partially useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation\n",
    "\n",
    "After having done this univariate analysis with only minimal modifications to the original data, one thing is for sure: **we have lots of missing data that cannot remain missing for purposes of modeling.** In fact, if I leave the missing data as `NaN`, I won't even be able to do bivariate exploration, let alone modeling! So we need to perform imputation on the data. \n",
    "\n",
    "There are only a handful of common imputation methods:\n",
    "\n",
    "1. **Listwise deletion.** This entails removing any records that have missing values. This is simple and has no nuanced statistical consequences: it's pretty clear that we're reducing $n$ and therefore statistical power, but that's it.  However, every single record in our dataset falls into this category - therefore, this is not an option.\n",
    "\n",
    "2. **Interpolation.** This is a broad term that encapsulates a few different processes, but effectively it refers to a process in which values considered \"near\" to the missing value are used to fill that empty slot in the data. Backfilling and forward-filling are pretty common approaches, referring to the process of taking the last (next) known value in the data and filling in missing values that come after (before) them for back (forward) filling. Another option is to do a linear regression using only nearby data points to find the best value. These are perfectly valid methods, especially if you have a low amount of missing values for a given feature and they are randomly distributed throughout the data. In addition, it requires that the data be ordered in some fashion, such as in time series data. None of this is the case for our data, however, so this isnt' going to work either.\n",
    "\n",
    "3. **Feature-specific replacement using descriptive statistics.** Use the mean or the median of the feature to fill in the missing values. This has the upside of maintaining the original sample mean (when filling with the mean) or maintaining the sample median (when filling with the median), but of course for whichever statistic is used to do the filling, the other one will be changed entirely.\n",
    "\n",
    "Since we know that these data are pretty right-skewed with fairly long tails at high (absolute) values, it would be improper to take the average of each feature and use that as the imputed (replacement) value for our missing data. As such, we'll use the median for now, but of course revisit this issue if the modeling requires it.\n",
    "\n",
    "Before we do that, though, it's become clear to me that there's little value in using features with a high percentage of missing values. I hadn't previously considered the impact of imputed data. As such, I'm going to change my threshold for dropping features from 85%+ null to 60%+, which will result in the dropping of `deferral_payments` and `deferred_income` in addition to the other features we already dropped. We'll also drop `email_address` as this doesn't provide useful information and, being a string feature, will only generate errors down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have to make sure we define which columns are features and which is the labels\n",
    "features = df.drop('poi', axis = 1)\n",
    "features_lowNull = df_lowNull.drop('poi', axis = 1)\n",
    "labels = df.loc[:, 'poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noNullReps = df.copy()\n",
    "\n",
    "features.drop(['email_address'], axis = 1, inplace = True)\n",
    "features_lowNull.drop(columns = [\"deferral_payments\", \"deferred_income\", \"email_address\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='median')\n",
    "\n",
    "features.loc[:,:] = imp.fit_transform(features)\n",
    "features_lowNull.loc[:,:] = imp.fit_transform(features_lowNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's take a look at the correlation coefficients across all of our features to see if there are any that are roughly collinear and thus can be reasonably removed.\n",
    "\n",
    "Since `DataFrame.corr()` ignores NaN values, we can safely use `df_noNullReps` for this to make sure we are getting the best correlation coefficients to our knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_noNullReps_corr = df_noNullReps.corr(min_periods = 10)\n",
    "df_noNullReps_corr\n",
    "#df_noNullReps_corr[df_noNullReps_corr > 0.90]#.sum()\n",
    "#df_noNullReps_corr[df_noNullReps_corr < -0.90]#.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like the only features that are correlated with $r > 0.90$ or $r < -0.90$ are `total_stock_value` and `exercised_stock_options`, as well as `restricted_stock_deferred` and `total_payments`.** This is a little suprising actually, as I would have guessed that `restricted_stock` would also be highly correlated with `total_stock_value`. It's not unreasonably correlated, at $r = 0.855$, but it's not quite as strong of a correlation as the others. The same is true of `restricted_stock_deferred` and `restricted_stock`, as well as `deferred_income` and `deferral_payments`. It looks like, if we so desired, we could drop the following:\n",
    "\n",
    "Feature to Keep | Feature to Drop\n",
    "--- | ---\n",
    "total_stock_value | exercised_stock_options\n",
    "total_payments | restricted_stock_deferred\n",
    "\n",
    "My current plan is to keep these features for now (although `restricted_stock_deferred` was already dropped earlier due to its high fraction of missing values), but since we'll be performing univariate tests to select the most useful features, I expect that they'll drop out (and if they don't, we'll know that something is possibly amiss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's look at pair plots to see if there are any relationships we should explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For the sake of visualization and simplicity, let's just look at the financial data first, \n",
    "    #then we'll look at the email data\n",
    "    \n",
    "financial_feats = ['bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
    "                   'exercised_stock_options', 'expenses', 'loan_advances', 'long_term_incentive',\n",
    "                   'other', 'restricted_stock', 'restricted_stock_deferred',\n",
    "                   'salary', 'total_payments', 'total_stock_value']\n",
    "email_feats = ['from_messages', 'from_poi_to_this_person',\n",
    "               'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_messages']\n",
    "\n",
    "sns.pairplot(features[financial_feats], plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n",
    "\n",
    "plt.suptitle(\"Financial Features Pair Plot - No Features Removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not seeing anything obvious here, other than the impact our approach of imputing with the median value has had on the data (unfortunate, but unavoidable). Let's take a look at the pair plot for the DataFrame with some high-null features removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "financial_feats_lowNull = ['bonus',\n",
    "                   'exercised_stock_options', 'expenses', 'long_term_incentive',\n",
    "                   'other', 'restricted_stock',\n",
    "                   'salary', 'total_payments', 'total_stock_value']\n",
    "\n",
    "sns.pairplot(features_lowNull[financial_feats_lowNull], plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n",
    "\n",
    "plt.suptitle(\"Financial Features Pair Plot - High-Null Features Removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, there's not much to say here beyond what's already been found via correlation analysis earlier. Let's look now at email-related features. We won't need to look at the high-null and low-null data separately, as we haven't dropped any email-related features (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(features[email_feats], plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n",
    "\n",
    "plt.suptitle(\"Email Features Pair Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[email_feats].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships here aren't particularly surprising, except perhaps the fact that the only mildly-correlated features are `to_messages` and `shared_receipt_with_poi`. I'm not entirely sure what that should mean, other than people who were part of email chains involving POIs also tended to be popular receivers of email in general? Curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering, Scaling, and Selection\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "The next step in our analytical journey is to see if there are any new features we can engineer in order to increase our model's performance. The one I'm going to try out, that came to me as I was exploring the data, is a counter variable that keeps track of how many times a person has financial data in the top 10% of all values for that feature. This should be robust to my imputation approach, as that utilized the median as a fill value which is by definition the 50% breakpoint for the feature.\n",
    "\n",
    "For example if an individual has 3 financial features that are each in the top 10%, then `top_finance = 3` for that person. **My hypothesis here is that POIs tended to be committing fraud at a grand scale and would thus have finances that reflect their lofty \"ambitions.\"**\n",
    "\n",
    "One thing I'm going to keep an eye on when testing this feature (assuming it survives feature selection) is whether it provides an improper level of predictive power for a single feature. I'm a little worried that, by using quantiles of all of the data in a given feature to produce this count, I'm subtly allowing for statistics leak in my workflow. In other words, I'm creating a feature that contains knowledge of all of the samples, regardless of whether they're assigned to the training or testing sets. However, given that this feature is a combination of the quantile data for a bunch of other features, this leakage is likely quite minimal in its impact on later scoring of models. Still, I'll have to keep an eye on it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_checker(col, q):\n",
    "    '''\n",
    "    This function is meant to be used with DataFrame.apply(axis = 1). It looks at each column of a pandas DataFrame\n",
    "    and compares the values in that column to the quantile values provided by quantiles. \n",
    "    It then changes all of the values in that column to False or True: \n",
    "    False if the value is less than the quantile value and True if it is greater than or equal to the quantile value. \n",
    "    \n",
    "    It also takes any non-financial feature and sets all values for that feature to np.nan.\n",
    "    \n",
    "    It does this for every column and then sums across the rows to come up with a count for each person in the data\n",
    "    as to how often they were in the top X percentile.\n",
    "    \n",
    "    col: pandas DataFrame column.\n",
    "    q: float. Provides the quantile values you are interested in (e.g. top 10% = 0.90).\n",
    "    '''\n",
    "    \n",
    "    #Checks to see if the values in col are majority-positive\n",
    "    if abs(col.quantile(q)) > abs(col.quantile(1-q)):\n",
    "        return col >= col.quantile(q)\n",
    "    #Checks to see if the values in col are majority-negative\n",
    "    else:\n",
    "        return col <= col.quantile(1-q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_counts = features[financial_feats].apply(quantile_checker, args = (0.9,))\n",
    "features['top_finance'] = fin_counts.sum(axis=1)\n",
    "\n",
    "fin_counts = features_lowNull[financial_feats_lowNull].apply(quantile_checker, args = (0.9,))\n",
    "features_lowNull['top_finance'] = fin_counts.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['top_finance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Given that I'd like to try out a variety of models for this problem, including ones that require feature scaling to be applied (e.g. SVM or k-NN), I'm going to scale these data. The example for scaling shown in class was done using `MinMaxScaler`; I'd prefer to use `RobustScaler` as it is *robust* to outliers, doesn't assume the data are normally distributed (they're really not, even with log-transforms), and utilizes the median and IQR of each feature for centering and scaling. By virtue of utilizing the median for imputation of each feature, I've significantly impacted the mean of the features while maintaining their medians, a fact that makes `RobustScaler` particularly appealing. \n",
    "\n",
    "The only concern I have right now is that I might be causing \"statistics leaking\" by effectively using knowledge of the entire dataset's distribution to scale all of the data, regardless of their assignment to training or test groups. I guess we'll see if we get an unrealistic result for our precision and recall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "features_scaled = features.copy()\n",
    "\n",
    "#Scale all of the features\n",
    "features_scaled.loc[:,:] = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I didn't see this coming! It looks like `RobustScaler` may not have been the best choice, in a way, for the features with lots of imputed values. Since I used a simple replacement process with the feature medians, I have a few features wherein the 25% and 75% quartile values are equivalent. As the formula for `RobustScaler` is $\\frac{x_i-Q_1(x)}{Q_3(x)-Q_1(x)}$, **we end up with a divide-by-zero issue for `RobustScaler` and thus see very large minimum and maximum values in the scaled data.** Let's take a look at our null counts again, I have a feeling that high fractions of missing values directly correlates with bad scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_noNullReps.isnull().sum(axis = 0)/len(df_noNullReps)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What features (pre-scaling) have the same values for 25% and 75% quartiles?\n",
    "quants_equal = features.quantile([0.25,0.75]).loc[0.25] == features.quantile([0.25,0.75]).loc[0.75]\n",
    "quants_equal[quants_equal.values].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the features with equivalent 25% and 75% quartiles, with their fractions of missing values also listed:\n",
    "\n",
    "Feature | % Missing Values\n",
    "--- | ---\n",
    "deferral_payments | 73.6%\n",
    "deferred_income | 66.7%\n",
    "director_fees | 88.9%\n",
    "loan_advances | 97.9%\n",
    "long_term_incentive | 54.9%\n",
    "restricted_stock_deferred | 88.2%\n",
    "\n",
    "**Based upon these results, I think we have a good argument here for dropping `long_term_incentive` from our lowNulls features.** So that's something to keep in mind for future analyses and modeling I may do: 50% missing values (if using imputation via a constant such as the feature median) is a good cutoff for excluding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull.drop(['long_term_incentive'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull_scaled = features_lowNull.copy()\n",
    "features_lowNull_scaled.loc[:,:] = scaler.fit_transform(features_lowNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this looks more accurate. **The only outlier here now (in terms of maximum value) is `from_messages`.** This appears to be reflective of the extreme difference between the original data's maximum and 75% quartile values. This is a little concerning and seems to throw this feature on to a different scale than the other features (in terms of order of magnitude), but at this point I'm willing to go forward and see what the models give us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "My intent is to do a relatively simplistic univariate test for feature selection, `SelectPercentile` with the default F-value scoring function for classification tasks, `f_classif`. That being said, I did research on estimator-based (e.g. decision tree) approaches to feature selection that would allow for determination of feature importances that would then allow for ranking the features so you know what to keep and what to discard. In doing this research, I identified `sklearn.feature_selection.RFE` as the best candidate for robust, iterative feature selection using any estimator with a `coef_` or `feature_importances_` attribute. However, I decided not to go this route ultimately, as we have a relatively small number of features (15 for the dataset wherein we droppped features with high fractions of missing values, and 20 for the dataset with full features), so it didn't seem like such a complex (and likely time-consuming) approach was necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only running selection of full dataset to ensure I know how the code works. When the full workflow is \n",
    "    #put into place, selection will only be done using training data\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "selector = SelectPercentile(score_func = f_classif, percentile = 75)\n",
    "selector.fit(features_scaled, labels)\n",
    "print \"F-scores (high nulls incl.):\", selector.scores_\n",
    "print \"\\nFeatures to include (high nulls incl.):\", features_scaled.iloc[:,selector.get_support(indices = True)].columns.values\n",
    "\n",
    "selector.fit(features_lowNull_scaled, labels)\n",
    "print \"\\n\\nF-scores (lowNull):\", selector.scores_\n",
    "print \"\\nFeatures to include (lowNull):\", features_lowNull_scaled.iloc[:,selector.get_support(indices = True)].columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is certainly interesting. When using `SelectPercentile` on the DataFrame that hasn't had any features dropped due to high amounts of missing values (`features`), **we see that some of those high-null-fraction features were selected! Specifically, I'm referring to the features `deferred_income` and `loan_advances`.** \n",
    "\n",
    "That's particularly crazy when you realize that `loan_advances` originally had more than 97% of its values missing prior to imputation! Doing some experimentation, we find that you have to set the `percentile` parameter below 55% before you get rid of `loan_advances`! That's surprising, given how many of the values in that feature are imputed (and since I just did straight replacement via the median, this means that more than 97% of values in that feature *are the same value*). **To my mind, this speaks to the value of just dropping these high-null features early on, as clearly my imputation approach is influencing the selector in a way I wasn't expecting** (or my intuition about the value of these features with a lot of missing values is *seriously* flawed!).\n",
    "\n",
    "**Likely this is reflective of the problem I identified earlier in scaling, wherein `RobustScaler` is pushing the value of features with a fraction of imputed values > 50% much higher than other feature values.** This is yet another piece of evidence that I shouldn't expect the high-null-fraction features to perform well in modeling, but we'll have to wait for the modeling step before fully ruling them out.\n",
    "\n",
    "**At this point, I'm making a decision: I'm dropping the high-null-fraction features from my analysis.** I just don't feel comfortable using the high-null-fraction features for modeling and would like to save myself some time in the tuning and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull_scaled = features_lowNull_scaled.iloc[:,selector.get_support(indices = True)]\n",
    "features_lowNull_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Tuning\n",
    "\n",
    "## Pandas to NumPy Transformation\n",
    "For the purposes of exploring our data, we've been working entirely in pandas DataFrames. Now that we're ready to embark upon model selection and hyperparameter tuning, it's time to switch over to numpy arrays, as this is sklearn's preferred data format.\n",
    "\n",
    "The process we'll follow simply involves checking to make sure that each feature in the DataFrame is of a single numeric data type and that the columns aren't considered of the `object` dtype (if they are, that means there are mixed types within the column). If I do find any hetereogeneous data types within a given column, I'll have to investigate what they are and see if type casting makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowNull_scaled.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're good to go! I believe `sklearn` is able to handle the bool dtype for the labels just fine. Now to pull out the features and labels as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numpy = features_lowNull_scaled.values\n",
    "labels_numpy = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Splitting for Cross-Validation\n",
    "\n",
    "Now let's split up our data into a testing and training set. As mentioned earlier, I'm only going to do single-fold cross-validation for speed reasons, and then later tighten up my evaluation in my final workflow by doing 1000-fold.\n",
    "\n",
    "As we have a pretty small number of samples (144) and a significant imbalance in the fractions of samples with different class labels (only 12.5% are POIs), and our samples are unordered, we'll use shuffled stratified splitting to preserve the fractions of different class labels in our training and testing sets. \n",
    "\n",
    "I'm also only going to use 10% of the data for testing, as we have such a small number of samples that I'm concerned about not having enough training data. I was thinking about doing 20% until I realized that the final testing of my model will be done with a 10% test set size for the project grading, so that seemed like the best bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features_numpy, labels_numpy, \n",
    "                                                                            test_size=0.1, random_state=None, \n",
    "                                                                            shuffle = True, \n",
    "                                                                            stratify = labels_numpy)\n",
    "\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Validation\n",
    "\n",
    "In order to spot-check the different models I'm going to try, and of course to optimize the model I ultimately choose to use, I'm going to need to define what scoring metrics I want to optimize and how I'll run my validation process. To that end, I plan to utilize the precision, recall, and f1-score. The first two are necessary for understanding my model's performance as unbalanced classes in the data disallow me from using the simple accuracy score metric. The third is useful as it's the harmonic mean of the other two (when the precision and recall are equally weighted) and thus is useful as a primary metric for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "First things first: I'm going to try a bunch of models with their default parameters. While this is a bit of a naive approach, we know from class that no amount of hyperparameter tuning is likely to improve a model as well as its built-in assumptions and our preprocessing of the data (and how appropriate that preprocessing was to the model at hand). Thankfully, we can iterate through a bunch of models given how relatively small our dataset is, and thus how quick I expect training and testing to go.\n",
    "\n",
    "I'm going to start with a few models that I don't think have a high chance of performing well, mainly because I want to get them out of the way and because, quite frankly, it feels good to consistently see improving scores as I change out models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "This one seems to be a particularly poor fit for the job, mainly because it assumes normally-distributed features and my univariate EDA seems to indicate that that is not the state of our features. Still, it's easy to understand and implement, so what's the harm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First things first, get our scoring and reporting mechanism ready\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Not POI', 'POI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB(priors = None)\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for GaussianNB:\\n\\n\", classification_report(labels_test, labels_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That could have gone better, but also could have gone worse.** As I stated before trying this one, I didn't expect miracles, so the low scores for the `poi = True` (positive) class aren't that surprising.\n",
    "\n",
    "Oh well, on to the next model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "Unlike NaiveBayes, this one is non-parametric (meaning that it assumes nothing about the underlying distribution of the features). I have higher hopes for this one, although it is so sensitive to local data distributions that I'm worried it will prove pretty weakly predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for kNN:\\n\\n\", classification_report(labels_test, labels_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I must say, this performed far better than I had expected!** I really thought that the local data structure sensitivity of kNN would result in poor performance for my data, yet here we are!\n",
    "\n",
    "Interesting side note: if I had only used the default value for `n_neighbors` (5), I would have predicted no POIs at all! Yet changing it to 2 gave me the best result I could find when varying that parameter. Also, the performance of this model is extremely sensitive to the training/testing split. When I re-split with a new random state, my scores plummeted. Clearly this is a good argument for multi-fold cross-validation!\n",
    "\n",
    "Surprisingly pleasant outcome! This is already quite good for the purposes of the class rubric (I only needed to get a precision and recall of at least 0.3), but why not finish the process and see if we can push that recall even higher (especially since we know things may shake out differently when we due multi-fold CV later)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "SVMs are a good option as they are also non-parametric but also because they are so flexible: being able to utilize the \"kernel trick\" and play with so many different kernels is a boon, to be sure. As such, I'm actually going to try a few different kernels herein myself. I know, I know: I said I wouldn't do any hyperparameter tuning yet. But the difference in results can be very drastic in some models for a single parameter, so I feel like I should do at least that before ruling out a whole model (e.g. like I did by playing around with `n_neighbors` for kNN earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "kernel_type = 'poly'\n",
    "clf = SVC(kernel = kernel_type, C = 1, gamma = 'auto', degree = 3, coef0 = 0.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for SVM ({}):\\n\\n\".format(kernel_type), classification_report(labels_test, labels_pred, \n",
    "                                                                             target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not as great as the kNN results, but not horrible either.** Given the large amount of parameters available for tuning (in addition to kernel type, there are 4 if you do `kernel = 'poly'` or `'sigmoid'`), it's possible that we'd find a good combination that can pump these results up. Still though, I'd be very surprised if it could perform as well as kNN, even with all of that tuning.\n",
    "\n",
    "Interesting to note: I really didn't get very good performance at all with `kernel = 'linear'` or `'rbf'`: they both had 0 True Positives. How curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Decision Trees are also non-parametric, but they have a lot more flexibility than the SVM approach in that they can take on all sort of crazy shapes at the decision boundary to make the model fit the training data. No \"kernel tricks\" needed here. That being said, there flexibility is part of why they suffer from frequent overfitting: they've fit the training data far too well to be generalizable and their predictive power is thus quite poor. Let's see if that's a problem we suffer from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=5, \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                            class_weight='balanced')\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for Decision Tree:\\n\\n\", classification_report(labels_test, labels_pred, \n",
    "                                                                                  target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Huh, I'm a little surprised by the lackluster performance here.** I suppose there's no reason to have assumed this would perform any better or worse than an SVM or even kNN, but still I expected either SVM or DT to do as well or better than kNN, honestly. This is something I'm starting to learn about machine learning: sometimes the simplest models can actually outperform all others! \n",
    "\n",
    "Now that I've stated that, let's throw out that concept altogether and try some ensemble models! First up, random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest improves upon the Decision Tree approach through what's known as bootstrap sampling/aggregation (or, specific to random forests, 'tree bagging'). The idea is that we generate a bunch of decision trees (a *forest*, if you will) by randomly choosing a subset of our training data with which to train each new tree. We also do this random selection approach when deciding what features will be considered for splitting at each leaf. Combined together, the multiple trees allows us to effectively \"average away\" the effects of overfitting that individual trees suffer from, and the bootstrapping of features and samples allows for the high correlation that would normally exist between the trees that we've randomly sampled to build. \n",
    "\n",
    "Since we played around with the hyperparameters a little bit earlier for the Decision Tree, let's start with those values that seemed to optimize it (for the parameters that are shared between the two models, that is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 50, max_depth=3, min_samples_split=2, \n",
    "                             min_samples_leaf=5, max_features='auto', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                             class_weight='balanced')\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for Random Forest:\\n\\n\", classification_report(labels_test, labels_pred, \n",
    "                                                                                  target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yikes, the performance of this one is quite dependent upon its choice of random state for bootstrapping.** The best result I saw is not the one shown right now, but there were plenty that did just as poorly. I can see the value, statistically, in a stochastic model like this one, but it really does make initial estimation like what I'm doing now pretty hard to do!\n",
    "\n",
    "Note that the value here for `n_estimators` is way higher than the default of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with Decision Trees\n",
    "\n",
    "OK, one more ensemble method and then we're done. AdaBoost with decision trees as its \"weak learners\" is what we'll be attempting here, although I may try one or two other estimators beyond decision trees depending upon how this goes. AdaBoost is effectively a sequential ensemble: unlike a random forest, AdaBoost is a bit more selective in how it works. It generates the first \"weak learner\" (in our initial case, a decision tree), checks to see how well it did fitting, then generates a new weak learner that is designed to properly classify the labels that the first one did poorly on. It proceeds like that for as long as it needs to (or to some cap value), constantly engineering a new weak learner each time to try and improve on the overall performance of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3, \n",
    "                                                               min_samples_split=2, min_samples_leaf=5,\n",
    "                                                              class_weight='balanced'),\n",
    "                         n_estimators=1000, learning_rate=1.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_pred = clf.predict(features_test)\n",
    "\n",
    "print \"Results for AdaBoost:\\n\\n\", classification_report(labels_test, labels_pred,\n",
    "                                                         target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Man, this performance is unimpressive to say the least.** Now, to be fair, I read in the sklearn documentation, I believe, that AdaBoost is pretty sensitive to outliers. Given that we have plenty of those, that might provide an explanation of why we're seeing such poor performance. Still, it's disappointing - complicated models make for more tuning fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drumroll, please...\n",
    "\n",
    "Based upon our brief search of the different models, it looks like k-Nearest Neighbors is the winner! It's really quite a simple model, so we won't have a whole lot of parameters to tune (only two, really), but still it's worth exploring it in some depth and getting the best possible performance we can. SVM performed equivalently, so we may revisit that and tune an SVM model (poly kernel) if we don't get the results we're hoping for in our full workflow with kNN. Onward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "At this point, we'll be tuning the kNN model's parameters `n_neighbors`, `weights`, and `p`:\n",
    "\n",
    "* The `p` parameter is one we didn't use previously in our model exploration step: it dictates whether the algorithm uses the Manhattan distance (`p = 1`) or the Euclidean distance (`p = 2`). In theory, we could also go to higher `p` values which would correspond to different power levels of the generalized Minkowski distance, but I don't really understand intuitively what those higher power values would be doing in kNN, so I prefer not to use it (I prefer to make decisions with machine learning in the same way as I do with investing: if you don't understand the technique/vehicle you're using, you shouldn't be using it).\n",
    "* The `n_neighbors` parameter dictates the number of neighboring data points in the feature space that should be used by default for determining the label of the data point being investigated at any given time. For example, a value of 4 for this parameter means that the 4 nearest neighbors of each data point are used to determine what the likely label of the data points being investigated are through a simple majority voting mechanism (for a classification problem such as this one).\n",
    "* The `weights` parameter can take on values of `'uniform'` or `'distance'`. The former means that all data points, regardless of their proximity to the point being investigated/voted upon, are given equal weight in the decision making process. As a result, the nearest of the near neighbors have the same influence on the label determination process as further neighbors do. When `weights = 'distance'`, the influence of neighbors is weighted by the inverse of their distance from the point being investigated. As a result, nearer neighbors have a strong influence when compared to farther neighbors.\n",
    "\n",
    "### Validation Process for Tuning\n",
    "\n",
    "In order to tune properly, I need to be a bit more thorough in cross-validation scheme than I was during the model selection phase of this workflow. To that end, I'm still going to use a stratified and shuffled training and testing set generation approach, but this time I'll do it 10 times over and average the resultant performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Suppress the warnings coming from GridSearchCV to reduce output messages\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': range(1,21,1),\n",
    "              'weights': ['uniform', 'distance'],\n",
    "             'p': [1,2]}\n",
    "\n",
    "#Setup cross-validator object so we have full control over the data we're tuning with\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state = 42)\n",
    "\n",
    "gs = GridSearchCV(KNeighborsClassifier(), param_grid, \n",
    "                   scoring = ['precision', 'recall', 'f1'], \n",
    "                  cv = cv, refit = 'f1', return_train_score = False)\n",
    "knn = gs.fit(features_numpy, labels_numpy).best_estimator_\n",
    "\n",
    "#Generate a results report\n",
    "params_and_metrics = ['mean_test_f1', 'mean_test_precision', 'mean_test_recall', \n",
    "                      'param_n_neighbors', 'param_p', 'param_weights']\n",
    "gridSearch_results_df = pd.DataFrame(gs.cv_results_)\n",
    "gridSearch_results_df[params_and_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridSearch_results_df.iloc[gs.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well this is a bit disappointing.** While $F_1=0.45$ is by no means the worst outcome, I was expecting something much higher based upon my previous single-split model exploration. While this is sufficient to pass the projct requirements (precision and recall > 0.30), I would like to get a bit more robust of an outcome. Let's see if an SVM gets us where we want to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Value gamma defaults to 'auto' = 1/n_features = 0.1 for these data\n",
    "#degree = 1 is basically the linear model with scalar multiplier gamma and an add'l intercept coef0\n",
    "param_grid = {'C': np.arange(0.1, 2.0, 0.3),\n",
    "             'gamma': [0.1,0.2,0.3],\n",
    "             'degree': [1,2,3,4,5]}\n",
    "\n",
    "gs = GridSearchCV(SVC(kernel = 'poly', coef0 = 0.0), param_grid, \n",
    "                   scoring = ['precision', 'recall', 'f1'], \n",
    "                  cv = cv, refit = 'f1', return_train_score = False)\n",
    "svm = gs.fit(features_numpy, labels_numpy).best_estimator_\n",
    "\n",
    "#Generate a results report\n",
    "params_and_metrics = ['mean_test_f1', 'mean_test_precision', 'mean_test_recall']\n",
    "for k in param_grid.keys(): params_and_metrics.append(\"param_\" + k)\n",
    "gridSearch_results_df = pd.DataFrame(gs.cv_results_)\n",
    "gridSearch_results_df[params_and_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridSearch_results_df.iloc[gs.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the SVM, we don't get a significantly increased f1-score ($F_1=0.47$), but we do get a more balanced precision and recall.** Here are the results for our two optimized models:\n",
    "\n",
    "Model | Precision | Recall | f1 Score\n",
    "--- | --- | ---\n",
    "kNN | 0.65 | 0.35 | 0.45\n",
    "SVM | 0.53 | 0.5 | 0.47\n",
    "\n",
    "So, it looks like we are able to get a result with a lower number of false positives (FP) but a higher number of False Negatives (FN) for the kNN model when compared to the SVM. So if our goal was to identify as many POIs as possible, even if we're found to be wrong in doing so for some of them later, the kNN model would be our preference, especially since its $F_1$ score is so close to the SVM model. However, if we were concerned about the consequences of naming someone as a POI incorrectly and would rather mischaracterize people as *not* being a POI more often, then we would go with the SVM. **Since I have no specific use case in mind for these results, I'll opt for the optimized SVM since it had the higher $F_1$ score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Validation\n",
    "\n",
    "OK, now we have all of our data processed, features selected, the model selected, and we've tuned said model. PHEW! Now for the final step: thorough cross-validation. I'm going to use the exact same strategy as I did during the hyperparameter tuning stage, but with 100x more iterations (total of 1,000), as this is the same strategy being used for final project testing and is clearly a thorough test of the algorithm. \n",
    "\n",
    "And, just in case it turns out my SVM model doesn't work out as well as I'm hoping, we've always got our kNN classifier if we need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=1000, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(svm, features_numpy, labels_numpy, groups=None, \n",
    "                        scoring=['precision', 'recall', 'f1'], cv=cv, \n",
    "                        return_train_score=False)\n",
    "cv_results = pd.DataFrame(scores)\n",
    "cv_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well, it's not as great as my smaller-iteration work during parameter tuning (which achieved an f1-score for the best estimator of 0.47), but it's not terrible!** Let's quickly see if we can do better with the kNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(knn, features_numpy, labels_numpy, groups=None, \n",
    "                        scoring=['precision', 'recall', 'f1'], cv=cv, \n",
    "                        return_train_score=False)\n",
    "cv_results = pd.DataFrame(scores)\n",
    "cv_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoa! That made the difference apparently. It looks like our k-Nearest Neighbors model is the ultimate winner!** With a higher mean recall, precision, and f1-score, this model clearly is the better of the two. OK, that does it for me. Now I'm off to answer some final questions for summarizing this work and then I'll figure out how to build everything we did here into a `Pipeline` for the final code submission..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up: Project Questions\n",
    "\n",
    "Here I'll answer the different questions posed by Udacity in the project rubric, to try and summarize what I've learned in this analysis and explain my thought process as best as I can. As I've done much of this in a Jupyter notebook, hopefully most of these questions have alreaady been answered earlier, but I'll cover them here too for the sake of completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  (relevant rubric items: “data exploration”, “outlier investigation”)\n",
    "\n",
    "The goal of this project is to discern a method for predicting who, among the list of Enron employees for whom we have financial and/or email data, would be eventually considered a Person of Interest (POI) by the SEC investigating Enron's fraudulent activities. I could see this having been a very useful exercise for the SEC back in the early 2000s when this investigation was actually occurring, as machine learning has enabled me to take a very limited amount of information about a relatively small number of people (especially small when you consider that the final list of POIs was only 12.5% of the overall samples in our data) and use it to do a halfway decent job predicting who would become a POI. That being said, our performance here was hardly mind-blowing, but I think it was quite good given the very limited data set.\n",
    "\n",
    "The data in question included a variety of financial data for the different individuals (e.g. director's fees they were paid, how much their Enron stock was worth, different payments they received from the company, etc.) as well as summary data about their company emails (e.g. count of messages they sent overall, number of messages they received from POIs, etc.). There were a few outliers in both kinds of data features (financial and email), but for the most part I allowed them to remain in the dataset as they did not appear to be due to errors of any kind and seemed like they were dispersed among enough different individuals that they may provide significant predictive value.\n",
    "\n",
    "The only outliers I did remove were samples that seemed outside the scope of our project's goal or that were clearly data entry errors. For example, I removed the record for 'THE TRAVEL AGENCY IN THE PARK' because it clearly referred to a party external to Enron that was utilized as a vendor for travel services and contained very little data besides. I also removed 'TOTAL' as that was a spreadsheet import error: it simply listed the total value for every financial column, making it an extreme outlier in all of the financial data effectively (and of course, not a real person!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### What features did you end up using in your POI identifier, and what selection process did you use to pick them? \n",
    "\n",
    "I ended up using a combination of manual and automated feature selection (`SelectPercentile`), starting from the full feature set and reducing from there. For the manual process, I first decided to drop features that contained more than 60% missing values, but eventually expanded that to features with more than 50% missing values. My initial reasoning was that genuine (non-imputed) data should be a significant majority of the features, so 60% was a reasonable value in my mind to achieve this. I expanded it to 50% because I noted that, after imputing the features' missing values using the feature-specific median (so as to maintain the IQR and median of each feature, due to the quantity of valid outliers I left in), I couldn't properly scale the features that had between 50% and 60% missing values (see below for my discussion of the feature scaling process I utilized).\n",
    "\n",
    "After this manual selection process, I utilized `SelectPercentile` with the ANOVA scoring function `f_classif` as an automated univariate feature selector. I selected only the top 75% of features. I initially investigated the top 50% of remaining features, but decided upon 75% when I realized that 50% didn't include any email features, only financial ones. I felt that removing an entire category of features would likely remove a large amount of useful data and, given that we had so few features to begin with, this seemed like a reasonable way to trim some excess information without losing too much. The features I ended up with as a result of using `SelectPercentile` are listed in the table below, with their corresponding F-scores included. Note that the feature `top_finance` is the one that I engineered for this project, and it will be explained below.\n",
    "\n",
    "Top 75% Features | F-Score\n",
    "--- | ---\n",
    "exercised_stock_options | 27.42\n",
    "total_stock_value | 23.68\n",
    "top_finance | 21.43\n",
    "bonus | 15.98\n",
    "salary | 10.97\n",
    "restricted_stock | 8.48\n",
    "total_payments | 8.41\n",
    "shared_receipt_with_poi | 7.48\n",
    "from_poi_to_this_person | 4.28\n",
    "other | 3.96\n",
    "\n",
    "\n",
    "### Did you have to do any scaling? Why or why not? \n",
    "\n",
    "As I chose not to be liberal in my removal of outlier data (only removing data that seemed erroneously entered instead of the \"remove top 10% of residuals from a linear regression\" approach due to the low amount of data and the relevance of each data point for each record), it became clear that feature scaling would be necessary. While some models (e.g. decision trees and ensembles of trees) are insensitive to scale mismatches across features, plenty of models (including SVM and kNN, my two highest-performing models) are sensitive to it. As I didn't want my preprocessing approach to determine what models I could use, I decided scaling was for the best. I chose to utilized `RobustScaler` for this purpose, as it is able to provide the value of scaling while preserving the impact of outliers. Unlike with `MinMaxScaler`, wherein you have a hard upper limit on the values of your scaled data in every feature (usually 1.0), `RobustScaler` is more flexible and thus can provide a more relevant comparison of truly large outliers vs. simply the maximum value of a feature that is not really an outlier unto itself.\n",
    "\n",
    "`RobustScaler` works by centering the data using the median of the feature and then scaling the data using the feature's interquartile range (IQR), assuming you enable both its centering and scaling functionalities. By doing this scaling, I made model comparisons between the email and financial data more feasible (as the maximum email feature value was on the order of 1E4, compared to 1E6 or 1E7 for financial features).\n",
    "\n",
    "### As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it\n",
    "\n",
    "I chose to engineer a feature that would count up how many features for a given sample fell into the top X quantile of that feature. I chose to set X = 0.90, effectively looking at the 90% (or top) decile of each financial feature. The logic to this was that those committing fraud at Enron were likely white collar criminals with a lot of access to Enron's financial backers, financial mechanisms (e.g. stock buybacks), etc. People with this kind of access are usually company executives and, the sad truth is, in America executives tend to be significant outliers when it comes to different types of compensation relative to the rest of their company's workforce. So it stood to reason that POIs would be at the top of the financial ladders. Ultimately this feature proved to be the third-most important one via `f_classif` testing with `SelectPercentile`, after removing features with a high fraction of missing values, so it looks like my intuition was valid!\n",
    "\n",
    "One thing to note with my approach to engineering this feature: there are some individuals in this dataset (e.g. `'BHATNAGAR SANJAY'`) who have a value for a few features that is very negative when the quantile value is positive, or vice versa. Due to the magnitude of the values for these individuals, my feature engineering code will actually consider them to be in the top quantile even if that quantile is defined by positive values and their value is negative. For example, BHATNAGAR SANJAY has -2604490.0 in `restricted_stock` and the quantile value for this feature is 1967034.8. While the value for this sample is clearly **much** less than the quantile value, my approach using the absolute value for comparisons of quantiles to feature values (in my `TopQuantile()` class below) is designed to increase that individual's counter due to that feature. At first I thought this was a bug in my code, but now I think it may be a feature: those large, oppositely-signed-to-the-quantile values probably *should* be included in the \"top quantile\" per my definition of it. Usually such large negative values indicate that someone has leveraged a category of their compensation in order to do something (e.g. take out a loan to buy more company stock). This is still a signature of the \"wheelers and dealers\" in Enron that I'm trying to identify, and as such I am leaving this functionality as it is.\n",
    "\n",
    "### In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  (relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”)\n",
    "\n",
    "Please see the first section of this question for a response to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  (relevant rubric item: “pick an algorithm”)\n",
    "\n",
    "I chose to try a variety of (mostly non-parametric) models. I effectively used a large number of the models mentioned in the course. I mostly stayed away from parametric models as I never saw a clear distribution come out of any of the underlying features: $log_{10}$ transformations of financial features didn't yield a Gaussian result, for example. As mentioned earlier, I didn't have to worry about model sensitivity to scaling as I applied feature scaling earlier in the workflow. \n",
    "\n",
    "I chose to evaluate my models primarily on their $F_1$ scores, but I also paid attention to their precision and recall, largely just so I could differentiate how two models might be performing differntly even if they had roughly equivalent $F_1$ scores. I chose to test each model with a single-fold stratified and shuffled cross-validation strategy and with only slight parameter tuning. As first-blush impressions were all I cared about this early in the modeling stage, I didn't feel the need to take a multi-fold approach.\n",
    "\n",
    "Given that this project is mainly interested in predicting the positive label (`poi = True`), the values I report here all focus on only the positive class. The performance metrics for the different models were:\n",
    "\n",
    "Model | Precision | Recall | F1 Score\n",
    "--- | --- | --- | ---\n",
    "GaussianNB | 0.00 | 0.00 | 0.00\n",
    "k-Nearest Neighbor | 1.00 | 0.50 | 0.67\n",
    "SVM (poly kernel) | 0.50 | 0.50 | 0.50\n",
    "Decision Tree | 0.25 | 0.50 | 0.33\n",
    "Random Forest | 0.00 | 0.00 | 0.00\n",
    "AdaBoost | 0.00 | 0.00 | 0.00\n",
    "\n",
    "Based upon this, I chose to tune only two of these models (picking two because their performance was reasonably similar): a polynomial-kernel SVM and a k-Nearest Neighbor model. More on the tuning can be found in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  \n",
    "\n",
    "Tuning the (hyper)parameters of a model is the process of optimizing the parameters you feed the model beyond just what it learns from the `fit(training_features, training_labels)` step. For example, in k-NN, there is an `n_neighbors` hyperparameter that is effectively asking the user \"how many nearest neighbors should I include in the vote to determine the likely class of the data point I'm looking at right now?\" Tuning that hyperparameter for k-NN can result in huge swings in the f1-score, and it's not even the only parameter to tune for that one (pretty simplistic) model! So you need to determine and implement a method for going through the hyperparameter space available to your model in order to figure out how you can tease out the best possible performance.\n",
    "\n",
    "If you don't do a good job tuning your hyperparameters, you can easily take a model with reasonable performance and cause it to perform worse than another optimized model. This is what makes the model selection process so tricky: you know one model might dominate another when tuned, but when left with its default values it may appear to underperform relative to the other model. Unless you build a pipeline in which multiple relevant models all get run (and tuned) as part of your model selection process (a very time-consuming endeavor for large datasets), it's impossible to know if you've found yourself in this situation. That's why, when I went about my model selection step, I manually tweaked a few of the most performance-influencing hyperparameters for each model to make sure that any poor performance I was seeing wasn't the fault of the default hyperparameter settings.\n",
    "\n",
    "### How did you tune the parameters of your particular algorithm? What parameters did you tune?\n",
    "\n",
    "I chose to do what's known as \"exhaustive grid searching\" for tuning my models via scikit-learn's `GridSearchCV`. I chose ranges of values for parameters I felt were the most likely to influence my performance significantly (this determination largely being made due to results observed during my quick runs for model selection) and fed these into my grid search algorithm, along with a cross-validator object to perform stratified and shuffled splitting of my testing and training sets for 10 folds. Here are the parameters I chose to tune for each model:\n",
    "\n",
    "* k-NN\n",
    "    * `n_neighbors`: I varied this in the range of 1 - 21 with integer steps\n",
    "    * `weights`: I varied this between its two options for weighting the importance of a neighbor's vote: 'uniform' (no weight given due to relative location of neighbor) and 'distance' (weighted according to the inverse of the relative distance of the neighbor to the data point in question)\n",
    "    * `p`: I varied this between its two most common options, 1 (Manhattan distance used) and 2 (Euclidean distance used)\n",
    "             \n",
    "* SVM (poly kernel)\n",
    "    * `C`: I varied this between 0.1 and 2.0 in steps of 0.3. This hyperparameter effectively determines how convoluted or smooth you want the decision boundary in feature space to be. Higher values allow for a better training fit, but run the risk of overfitting. Lower values run the risk of underfitting.\n",
    "    * `gamma`: this was varied from 0.1 to 0.3  in steps of 0.1. This is effectively just a scalar multiplier for the kernel function.\n",
    "    * `degree`: took on the values of 1 through 5 in integer steps. This determines, specfically for the polynomial kernel I used, how high of an order/degree the polynomial is allowed to reach. So, at the highest value I used, the kernel was allowed to go up to $X^5$.\n",
    "    \n",
    "    \n",
    "I've reproduced the results of my hyperparameter tuning here for the sake of clarity and summing things up:\n",
    "\n",
    "<p style=\"text-align: center;\"> *Hyperparameter Tuning Results* </p>\n",
    "Model | Precision | Recall | f1 Score\n",
    "--- | --- | ---\n",
    "kNN | 0.65 | 0.35 | 0.45\n",
    "SVM | 0.53 | 0.5 | 0.47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### What is validation, and what’s a classic mistake you can make if you do it wrong? \n",
    "\n",
    "Validation, or more specifically cross-validation, is the process of splitting your data into one or more training and testing sets. The idea is that you separate the data in this way to make sure your model has data it can be validated against (the test set) without ever having seen those data before and learned from them when the model was being created. Since the whole point of machine learning is to predict things (or probabilities of things), we can't very well use our test set to also train our model, right? While the model may not predict the test set perfectly, it certainly will do much better than it has any right to do. The ultimate goal here is to simulate situations your model will be facing later on when it is asked to make a prediction on data that it has truly never seen before and with no labels provided (because they're not known yet).\n",
    "\n",
    "There are two classic mistakes I can think of when it comes to cross-validation. First is the most obvious: you can train your model on the same data as that which you test it on and get an artificially inflated predictive accuracy. Variants of this mistake include splitting the data into testing and training sets but then testing it on the training data (out of confusion most likely). The other classic mistake tends to be more nuanced: there are a large variety of ways to split your data, and you may not choose the most relevant strategy for the dataset on hand. For example, you may choose to randomly select samples to go into your training and testing groups, but this can be an error for time series data (if you want to predict future trends, you had better not randomly put future events into the training data!). More discussion on this approach, for these specific data, is given in the next response below.\n",
    "\n",
    "### How did you validate your analysis? \n",
    "\n",
    "Cross-validation can be done in a surprisingly large number of ways, and I know I've only scratched the surface. Thankfully, the folks at Udacity were kind enough to include their testing algorithm that they'll be using to generate part of my grade on this, as that exposed me to the concept of stratified cross-validation. Stratification involves more than just choosing what data will be in your test and training sets: it forces the fractions of samples with different labels/classes in each data set to be the same as the overall dataset. So in other words, if you have a binary classification situation wherein 12.5% of your samples are positively labeled and 87.5% are negatively labeled, stratification would ensure that all training and testing sets maintain this 12.5/87.5 proportion for the different classes. **In other words, stratification is an important aspect to include in your cross-validation if you have a small number of samples and the labels are unbalanced.** Now, if you have a large number of samples, even with unbalanced labels, chances are good that you'd be just fine with random selection of testing and training sets, but that was not the case for the Enron data.\n",
    "\n",
    "I also shuffled my data as it is helpful to randomize away any bias that may exist in the training/testing splitting method. This was a valid approach to take with these data, as samples are not ordered in any meaningful way, so shuffling up the order should have no negative consequences.\n",
    "\n",
    "For my final model validation step, I decided to mimic the parameters used in the Udacity tester algorithm and run 1000 folds then average the result of the precision, recall, and f1-score for each fold. I was surprised to find that, upon increasing the number of folds relative to my hyperparameter tuning step, I actually saw the dominant model become the underperforming one! This says to me that the two models are likely performing very similarly and can be used interchangeably (or perhaps used for specific situations wherein higher recall or precision is needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "Here are the results of my final validation and evaluation of my two models:\n",
    "\n",
    "<p style=\"text-align: center;\"> *Final Model Mean Evaluation Results* </p>\n",
    "Model | Precision | Recall | f1 Score\n",
    "--- | --- | ---\n",
    "kNN | 0.53 | 0.36 | 0.41\n",
    "SVM | 0.36 | 0.35 | 0.33\n",
    "\n",
    "As you can see here, I chose to use the same metrics throughout, starting with my hyperparameter tuning all the way through to final model evaluation/validation: namely, precision, recall, and f1 score (the harmonic mean of precision and recall). I was effectively trying to maximize the f1 score above all else, but the final performance of the kNN model ended up being superior to the SVM on all three metrics, making the choice of which model was \"optimal\" pretty clear. It's interesting because the dominant model at the end of hyperparameter tuning looked to be the SVM, yet the kNN supplanted it at the final validation stage. This suggests to me that, if we ran the stochastic cross-validation scheme a few different times (without specifying a static random state seed value), we'd likely find that the two models switched dominant positions.\n",
    "\n",
    "So, it looks like we are able to get a result with a lower number of false positives (FP) with the kNN model and about an equivalent number of False Negatives (FN) when compared to the SVM. So **if our goal were to identify as many POIs as possible, even if we're found to be wrong in doing so for some of them later, the kNN model would be our preference.** However, **if we were concerned about the consequences of naming someone as a POI incorrectly and would rather mischaracterize people as not being a POI more often, then we could go with either model**, since their Recall values are nearly the same (of course, I'm speculating a little bit here, since it's possible that one has a low False Negative count and one has a low True Positive count that simply balanced out with its higher False Negatives, but that seems unlikely given the Precision values for both models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class TopQuantile(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Engineer a new feature using the top quantile values of a given set of features. \n",
    "    \n",
    "    For every value in those features, check to see if the value is within the top q-quantile\n",
    "    of that feature. If so, increase the count for that sample by +1. New feature is an integer count\n",
    "    of how often each sample had a value in the top q-quantile of the specified features.\n",
    "    \n",
    "    This class's fit(), transform(), and fit_transform() methods all assume a pandas DataFrame as input.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, new_feature_name = 'top_finance', feature_list = None, q = 0.90):\n",
    "        '''\n",
    "        Constructor for TopQuantile objects. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        new_feature_name: str. Name of the feature that will be added as a pandas DataFrame column\n",
    "                            upon transformation.\n",
    "        \n",
    "        feature_list: list of str. Names of feature columns that should be included in \n",
    "                        the count of top quantile membership.\n",
    "                        \n",
    "        q: float. Corresponds to the percentage quantile you want to be counting for. For example,\n",
    "            q = 0.90 looks at the 90% percentile (top decile).\n",
    "        '''\n",
    "        self.new_feature_name = new_feature_name\n",
    "        self.feature_list = feature_list\n",
    "        self.q = q\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        '''\n",
    "        Calculates the q-quantile properly both for features that are largely positive\n",
    "        and ones that are largely negative (as DataFrame.quantile() does not do this correctly).\n",
    "        For example, if most of a feature's data points are between (-1E5,0), the \"top decile\"\n",
    "        should not be -100, it should be -1E4.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: features DataFrame or numpy array, one feature per column\n",
    "        y: labels DataFrame/numpy array, ignored\n",
    "        '''\n",
    "        \n",
    "        Are \n",
    "        if isinstance(X, [pd.DataFrame]):\n",
    "            #Majority-negative features need to check df.quantile(1-q)\n",
    "                #in order to be using correct quantile value\n",
    "            pos = X.loc[:,self.feature_list].quantile(self.q)\n",
    "            neg = X.loc[:,self.feature_list].quantile(1.0-self.q)\n",
    "\n",
    "            #Replace negative quantile values of neg within pos to create \n",
    "                #merged Series with proper quantile values for majority-positive\n",
    "                #and majority-negative features\n",
    "            pos.loc[neg < 0] = neg.loc[neg < 0]\n",
    "            self.quants = pos\n",
    "        \n",
    "        #Are features a NumPy array?\n",
    "        elif isinstance(X, [np.ndarray]):\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            raise TypeError('Features need to be either pandas DataFrame or numpy array')\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Using quantile information from fit(), adds a new feature to X that contains integer counts\n",
    "        of how many times a sample had a value that was in the top q-quantile of its feature, limited\n",
    "        to only features in self.feature_list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: features DataFrame or numpy array, one feature per column\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Input DataFrame with additional column for new_feature, called self.new_feature_name\n",
    "        '''\n",
    "        #Change all values in X to True or False if they are or are not within the\n",
    "            #top q-quantile\n",
    "        self.boolean_df = X.loc[:,self.feature_list].abs() >= self.quants.abs()\n",
    "        \n",
    "        #Sum across each row to produce the counts\n",
    "        X[self.new_feature_name] = self.boolean_df.sum(axis = 1)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        '''\n",
    "        Provides the identical output to running fit() and then transform() in one nice little package.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: features DataFrame, one feature per column\n",
    "        y: labels DataFrame, ignored\n",
    "        '''\n",
    "        \n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "    \n",
    "#Need to figure out how to handle the two cases of input being a DataFrame vs. numpy array\n",
    "    #If DataFrame, the code is just fine\n",
    "    #If numpy, not much needs to change, but how do I select features to include/exclude without names?\n",
    "        #Also, how to insert new feature efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_features = features.drop(\"top_finance\", axis=1).copy()\n",
    "\n",
    "feats = ['salary', 'deferral_payments', 'total_payments',\n",
    "         'exercised_stock_options', 'bonus', 'restricted_stock',\n",
    "         'restricted_stock_deferred',\n",
    "         'total_stock_value', 'expenses', 'loan_advances',\n",
    "         'other', 'director_fees',\n",
    "         'deferred_income', 'long_term_incentive']\n",
    "\n",
    "topFinance = TopQuantile(feature_list = feats)\n",
    "#newObj.fit(test_features)\n",
    "#test_features.loc[:,:] = newObj.transform(test_features)\n",
    "test_features.loc[:,:] = topFinance.fit_transform(test_features)\n",
    "\n",
    "test_features['top_finance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Plan\n",
    "\n",
    "1. **Import pickle file as data_dict** \n",
    "1. **Push data_dict into DataFrame**\n",
    "\n",
    "**Pipes knn and svm - untuned classifiers**\n",
    "1. **Imputation:** Effectively, I can't use the median of the data as the replacement value in imputation when I'm calculating the median using the whole dataset: I need to calculate it only on the training data and then apply it as the replacement value on the training and test sets (so never including the test data in the median calculation). This is the most appropriate way of avoiding leaking information about the test distribution into the training distribution (while they are expected to be coming from the same distribution, and thus there is an argument to be made for just taking the whole-dataset median, it feels intuitively safer and more valid to keep the training and test data statistics separate in this way).\n",
    "    * No need to replace 'NaN' with np.nan when using the `Imputer` object. \n",
    "4. **Feature engineering:** since my new feature depends upon knowledge of quantiles and therefore relies upon more than a single record to produce that record's new feature value, I need to calculate the quantiles only using the training data, then pass along the quantile breakpoints to both the training and testing data for transformation.\n",
    "    * Use my brand new `TopQuantile` class!\n",
    "5. **Feature scaling:** use the training set to fit the data to a `scaler` object, then transform training and testing data separately using the info learned from the training data.\n",
    "6. **Feature selection:** features need to be selected using *only* the training data, but of course dropping features will then affect all records equally.\n",
    "7. **Hyperparameter tuning:** I'll need to do a `FeatureUnion` here most likely, so that I can tune both an SVM and a kNN model in parallel.\n",
    "    * Use 10-fold CV here\n",
    "    * `FeatureUnion` will need to continue through final validation\n",
    "\n",
    "**Pipes knn_tuned and svm_tuned**\n",
    "1. **Final model validation:** run 1000-fold CV on both SVM and kNN models to determine the winner\n",
    "    * Using `best_estimator_` that came as a result of hyperparameter tuning for each model\n",
    "\n",
    "\n",
    "Ultimately, we must follow the maxim of machine learning: \"anything that is learned, must be learned using only training data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import the pickle file afresh and push it to a DataFrame\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "'''\n",
    "We'll grab all of the features except the ones we know are more than 50% missing values\n",
    "Specifically, we're not going to pull in:\n",
    "    1. loan_advances (97% missing)\n",
    "    2. director_fees (88%)\n",
    "    3. restricted_stock_deferred (87.7%)\n",
    "    4. deferral_payments (73%)\n",
    "    5. deferred_income (66%)\n",
    "    6. long_term_incentive (54.9%)\n",
    "    \n",
    "Also, we won't pull in email_address as that doesn't give us any useful info\n",
    "'''\n",
    "features_list = ['poi', 'salary', 'total_payments', 'bonus', \n",
    "                 'total_stock_value', \n",
    "                 'expenses', 'exercised_stock_options', 'other',\n",
    "                 'restricted_stock', \n",
    "                 'to_messages', 'from_poi_to_this_person', \n",
    "                 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "df.drop(columns = ['loan_advances', 'director_fees', 'restricted_stock_deferred',\n",
    "                  'deferral_payments', 'deferred_income', 'long_term_incentive',\n",
    "                  'email_address'], inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the two records we know aren't useful: 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK'\n",
    "df.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Imputer, RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "#Suppress the warnings coming from GridSearchCV to reduce output messages\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "features = df.drop(columns = ['poi'])\n",
    "labels = df['poi']\n",
    "\n",
    "#--------------------------------- CROSS-VALIDATION -----------------------------------------\n",
    "#Shuffled and stratified cross-validation binning for this tuning exercise\n",
    "cv_10 = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state = 42)\n",
    "\n",
    "#--------------------------------- IMPUTATION -----------------------------------------\n",
    "#Imputation using the median of each feature\n",
    "imp = Imputer(missing_values='NaN', strategy='median')\n",
    "\n",
    "#--------------------------------- FEATURE ENGINEERING -----------------------------------------\n",
    "#Feature Engineering with TopQuantile() to count the top quantile financial features\n",
    "feats = ['salary', 'total_payments', 'bonus', 'total_stock_value', 'expenses', \n",
    "         'exercised_stock_options', 'other', 'restricted_stock']\n",
    "\n",
    "topQ = TopQuantile(feature_list = feats)\n",
    "\n",
    "#--------------------------------- FEATURE SCALING -----------------------------------------\n",
    "#Feature Scaling via RobustScaler()\n",
    "scaler = RobustScaler()\n",
    "\n",
    "#--------------------------------- FEATURE SELECTION -----------------------------------------\n",
    "#Feature Selection via SelectPercentile(f_classif, percentile = 75)\n",
    "selector = SelectPercentile(score_func = f_classif, percentile = 75)\n",
    "\n",
    "#--------------------------------- TUNING -----------------------------------------\n",
    "#FeatureUnion to keep track of kNN and SVM model results\n",
    "knn = KNeighborsClassifier()\n",
    "knn_param_grid = {'kNN__n_neighbors': range(1,21,1), 'kNN__weights': ['uniform', 'distance'],\n",
    "                  'kNN__p': [1,2]}\n",
    "\n",
    "svm = SVC(kernel = 'poly')\n",
    "svm_param_grid = {'SVM__C': np.arange(0.1, 2.0, 0.3), 'SVM__gamma': [0.1,0.2,0.3], \n",
    "                  'SVM__degree': [1,2,3,4,5]}\n",
    "\n",
    "#Hyperparameter tuning\n",
    "#Apparently you're supposed to put the pipe in as a param for GridSearchCV, not the other way around...\n",
    "\n",
    "knn_pipe = Pipeline([('impute', imp), ('engineer',topQ), ('scale', scaler),\n",
    "                    ('select', selector), ('kNN', knn)])\n",
    "\n",
    "knn_gs = GridSearchCV(knn_pipe, knn_param_grid, scoring = ['precision', 'recall', 'f1'], \n",
    "                      cv = cv_10, refit = 'f1', return_train_score = False)\n",
    "knn_gs.fit(features, labels)\n",
    "results_df_knn = pd.DataFrame(knn_gs.cv_results_)\n",
    "\n",
    "svm_pipe = Pipeline([('impute', imp), ('engineer',topQ), ('scale', scaler),\n",
    "                    ('select', selector), ('SVM', svm)])\n",
    "\n",
    "svm_gs = GridSearchCV(svm_pipe, svm_param_grid, scoring = ['precision', 'recall', 'f1'], \n",
    "                      cv = cv_10, refit = 'f1', return_train_score = False)\n",
    "svm_gs.fit(features, labels)\n",
    "results_df_svm = pd.DataFrame(svm_gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "svm = gs.fit(features_numpy, labels_numpy).best_estimator_\n",
    "\n",
    "#Generate a results report\n",
    "params_and_metrics = ['mean_test_f1', 'mean_test_precision', 'mean_test_recall']\n",
    "for k in param_grid.keys(): params_and_metrics.append(\"param_\" + k)\n",
    "gridSearch_results_df = pd.DataFrame(gs.cv_results_)\n",
    "gridSearch_results_df[params_and_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Validation\n",
    "#Using the best_estimator_ output, re-build pipelines for the two models \n",
    "    #that put them through 1000-fold stratified shuffled cv (with no tuning, since that's done already)\n",
    "#This pipeline is the one that will going into the classifier pickle file for the final submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputting Everything into the Udacity Code Structure\n",
    "\n",
    "My workflow with pipelines doesn't make a whole lot of sense in the Udacity code structure, as they want to load in a single dataset and estimator and then test it. What I'll need to do is only pick one of the two models, based upon which one performs best here, then build my pipeline around that. The best estimator that comes out of the grid search done in this notebook will be what I hard-code in as the estimator in my pipeline, so the entire pipeline can be treated as my \"classifier\". \n",
    "\n",
    "HOWEVER, since my feature engineering step assumes a DataFrame will be the input and they will instead be passing my pipeline a NumPy array, I'll need to do imputation and feature engineering on the dataset without cross-validation binning, and then start the pipe at the featuer scaling step. Not ideal, but necessary to abide by the project construct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_MiniProjects]",
   "language": "python",
   "name": "conda-env-ML_MiniProjects-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
